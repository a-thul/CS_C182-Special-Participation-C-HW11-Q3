{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer Interpretability**\n",
    "\n",
    "In this coding homework, you will:\n",
    "* Implement a single attention head\n",
    "* Implement an induction copy head by combining a previous token head with a copying head.\n",
    "\n",
    "---\n",
    "\n",
    "## Refactored Version\n",
    "\n",
    "This notebook uses the refactored `transformer_interpretability` package which follows:\n",
    "- **PEP 8**: Python style conventions\n",
    "- **PEP 484**: Type hints\n",
    "- **Google Python Style Guide**: Code organization and docstrings\n",
    "- **NumPy Style Guide**: Documentation format\n",
    "\n",
    "See `REFACTORING_REPORT.md` for detailed documentation of changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports & Preliminaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Add parent directory to path for local imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from transformer_interpretability import single_attention_head, induction_copy_head\n",
    "from transformer_interpretability.utils.constants import DEFAULT_SEED, EPSILON\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(DEFAULT_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 1: A Single Attention Head**\n",
    "\n",
    "In this question, you'll implement an attention head.\n",
    "\n",
    "### Specifications\n",
    "\n",
    "- The query-key matrices are provided already multiplied together\n",
    "  (i.e., we provide as input $W_{QK} = W_Q^\\top W_K$, called `wqk`).\n",
    "- The output-value matrices are provided already multiplied together\n",
    "  (i.e., we provide as input $W_{OV} = W_O^\\top W_V$, called `wov`).\n",
    "- Attention inputs are also provided as a list of `d_model`-length vectors,\n",
    "  called `attn_input`. The list elements correspond to positions in the context.\n",
    "- The desired outputs are the outputs the attention head produces at each position.\n",
    "  This should be a list of `d_model`-length vectors of the same length as the input.\n",
    "- **Causal masking:** When implementing attention, mask out positions that come\n",
    "  after the current position by setting their attention scores to negative\n",
    "  infinity (e.g., `-1e9`) before applying softmax.\n",
    "- You should first convert the inputs to numpy arrays as a first step.\n",
    "\n",
    "### Note\n",
    "\n",
    "- You should **not** use `np.softmax` when calculating the attention scores.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The implementation is in `transformer_interpretability/core/attention.py`.\n",
    "\n",
    "Key functions:\n",
    "- `single_attention_head(attn_input, wqk, wov)` - Main attention computation\n",
    "- `softmax(logits, axis)` - Numerically stable softmax\n",
    "- `create_causal_mask(seq_len)` - Generate causal attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the function signature and docstring\n",
    "help(single_attention_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `single_attention_head` - Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test inputs\n",
    "attn_input = [\n",
    "    [0, 1],\n",
    "    [1, 1],\n",
    "    [1, 2],\n",
    "]\n",
    "wqk = [\n",
    "    [1, 1],\n",
    "    [0, 0],\n",
    "]\n",
    "wov = [\n",
    "    [1, 1],\n",
    "    [0, 0],\n",
    "]\n",
    "\n",
    "expected_output = [\n",
    "    [1.0, 0.0],\n",
    "    [1.7310585786300048, 0.0],\n",
    "    [2.5752103826044417, 0.0],\n",
    "]\n",
    "\n",
    "# Run the attention head\n",
    "output = single_attention_head(attn_input, wqk, wov).tolist()\n",
    "\n",
    "# Verify output\n",
    "assert np.isclose(expected_output, output, atol=EPSILON).all(), (\n",
    "    f\"Failed:\\nExpected: {expected_output}\\nGot: {output}\"\n",
    ")\n",
    "print(\"Test case passed ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `single_attention_head` - Full Suite\n",
    "\n",
    "Run against all provided test cases from the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test cases\n",
    "test_file = Path(\"tests/test_cases/single_attention_head_test_cases.json\")\n",
    "\n",
    "if test_file.exists():\n",
    "    with open(test_file) as f:\n",
    "        test_cases = json.load(f)\n",
    "\n",
    "    # Run all test cases\n",
    "    for test_case_id, test_data in test_cases.items():\n",
    "        attn_input, wqk, wov, expected_output = test_data\n",
    "        output = single_attention_head(attn_input, wqk, wov).tolist()\n",
    "        case_num = test_case_id.split()[-1]\n",
    "\n",
    "        assert np.isclose(expected_output, output, atol=EPSILON).all(), (\n",
    "            f\"Test Case {case_num} Failed:\\nExpected: {expected_output}\\nGot: {output}\"\n",
    "        )\n",
    "\n",
    "    print(\"All test cases passed ✅\")\n",
    "else:\n",
    "    print(f\"⚠️  Test file not found: {test_file}\")\n",
    "    print(\"Run from the correct directory or clone the course repository.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 2: An Induction Copy Head**\n",
    "\n",
    "In this problem, you will combine a previous token head with a copying head.\n",
    "\n",
    "### Background\n",
    "\n",
    "An induction head operates by predicting that previously-seen adjacencies in\n",
    "the sequence will be seen again. That is, it predicts `ab...a` will be\n",
    "followed by `b` (for any `a`, `b`).\n",
    "\n",
    "### Specifications\n",
    "\n",
    "#### Vocabulary and Embeddings\n",
    "- Vocabulary size: 4\n",
    "- Tokens: `a`, `b`, `c`, `d`\n",
    "- Maximum sequence length: 5\n",
    "- Embedding: 2-hot encoded with `d_model = 9`\n",
    "  - First 4 dimensions: 1-hot encoding of token vocabulary\n",
    "  - Next 5 dimensions: 1-hot encoding of position (0-4)\n",
    "- The unembeddings are the same as the embeddings. The model produces an\n",
    "  output vector of length 4 which encodes the logits on `a,b,c,d` respectively.\n",
    "\n",
    "#### Induction Head Mechanism\n",
    "Your implementation should consist of two stages:\n",
    "\n",
    "1. **Previous Token Head:** Identifies tokens that are directly adjacent to\n",
    "   each other (using position information)\n",
    "2. **Induction Head:** Takes the output from the previous token head and\n",
    "   copies the token that follows matching patterns.\n",
    "\n",
    "Together, these implement the pattern: `a,b,...,a -> b` for all `a`, `b`.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The implementation is in `transformer_interpretability/core/induction_heads.py`.\n",
    "\n",
    "Key functions:\n",
    "- `induction_copy_head(embeddings, attention_strength)` - Main induction mechanism\n",
    "- `_build_previous_token_matrices(d_model, attention_strength)` - Construct PTH weights\n",
    "- `_build_copying_matrices(d_model, vocab_size, attention_strength)` - Construct copy head weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Walkthrough\n",
    "\n",
    "For the sequence `[a, b, c, d, a]`, the prediction should upweight `b`.\n",
    "\n",
    "**Step-by-step:**\n",
    "\n",
    "1. **Previous Token Head** creates outputs where position $i$ contains\n",
    "   information about the token at $i - 1$.\n",
    "\n",
    "2. After deleting position 0 and adding to token embeddings (without position\n",
    "   info), we have representations that know both \"what token is here\" and\n",
    "   \"what token came before\".\n",
    "\n",
    "3. **Induction Head** at the final position sees token `a` and looks for\n",
    "   where else `a` appeared with its predecessor information.\n",
    "\n",
    "4. It finds that position 0 had token `a` (with no predecessor).\n",
    "\n",
    "5. It copies what came after position 0, which is token `b`.\n",
    "\n",
    "6. The output logits should thus have the highest value for token `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the function signature and docstring\n",
    "help(induction_copy_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `induction_copy_head` - Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test inputs: sequence [a, b, c, d, a]\n",
    "embeddings = [\n",
    "    [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # a at position 0\n",
    "    [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],  # b at position 1\n",
    "    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],  # c at position 2\n",
    "    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0],  # d at position 3\n",
    "    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # a at position 4\n",
    "]\n",
    "attention_strength = 10.0\n",
    "\n",
    "# Run the induction head\n",
    "output = induction_copy_head(embeddings, attention_strength).tolist()\n",
    "\n",
    "# Expected: high probability for 'b' (index 1)\n",
    "expected_output = [0.000045, 0.999864, 0.000045, 0.000045]\n",
    "\n",
    "# Verify output\n",
    "assert np.isclose(expected_output, output, atol=EPSILON).all(), (\n",
    "    f\"Failed:\\nExpected: {expected_output}\\nGot: {output}\"\n",
    ")\n",
    "print(\"Test case passed ✅\")\n",
    "print(f\"\\nPredicted token probabilities:\")\n",
    "print(f\"  a: {output[0]:.6f}\")\n",
    "print(f\"  b: {output[1]:.6f}  ← highest (correct!)\")\n",
    "print(f\"  c: {output[2]:.6f}\")\n",
    "print(f\"  d: {output[3]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `induction_copy_head` - Full Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test cases\n",
    "test_file = Path(\"tests/test_cases/induction_head_test_cases.json\")\n",
    "\n",
    "if test_file.exists():\n",
    "    with open(test_file) as f:\n",
    "        test_cases = json.load(f)\n",
    "\n",
    "    # Run all test cases\n",
    "    for test_case_id, test_data in test_cases.items():\n",
    "        embeddings, attention_strength, expected_output = test_data\n",
    "        output = induction_copy_head(embeddings, attention_strength).tolist()\n",
    "        case_num = test_case_id.split()[-1]\n",
    "\n",
    "        assert np.isclose(expected_output, output, atol=EPSILON).all(), (\n",
    "            f\"Test Case {case_num} Failed:\\nExpected: {expected_output}\\nGot: {output}\"\n",
    "        )\n",
    "\n",
    "    print(\"All test cases passed ✅\")\n",
    "else:\n",
    "    print(f\"⚠️  Test file not found: {test_file}\")\n",
    "    print(\"Run from the correct directory or clone the course repository.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we explored transformer interpretability concepts:\n",
    "\n",
    "1. **Single Attention Head**: Implemented causal self-attention with:\n",
    "   - Pre-multiplied QK and OV matrices\n",
    "   - Causal masking to prevent attending to future positions\n",
    "   - Numerically stable softmax\n",
    "\n",
    "2. **Induction Copy Head**: Combined two specialized heads:\n",
    "   - Previous Token Head: Uses positional encoding to copy predecessor info\n",
    "   - Copying Head: Matches patterns and copies following tokens\n",
    "\n",
    "These mechanisms are fundamental to understanding how transformers perform\n",
    "in-context learning.\n",
    "\n",
    "### References\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al., 2017\n",
    "- [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) - Elhage et al., 2021\n",
    "- [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) - Olsson et al., 2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
